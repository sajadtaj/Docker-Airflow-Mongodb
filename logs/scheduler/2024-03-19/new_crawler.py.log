[2024-03-19T09:48:50.359+0000] {processor.py:161} INFO - Started process (PID=2838) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T09:48:50.360+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T09:48:50.362+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:48:50.362+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:48:50.464+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:48:50.815+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:48:50.814+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T09:48:50.840+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:48:50.840+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-18 00:00:00+00:00, run_after=2024-03-18 01:00:00+00:00
[2024-03-19T09:48:50.862+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.507 seconds
[2024-03-19T09:49:14.756+0000] {processor.py:161} INFO - Started process (PID=2895) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T09:49:14.757+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T09:49:14.758+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:49:14.758+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:49:14.970+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:49:14.978+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:49:14.978+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T09:49:14.993+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:49:14.992+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-18 00:00:00+00:00, run_after=2024-03-18 01:00:00+00:00
[2024-03-19T09:49:15.008+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.254 seconds
[2024-03-19T09:49:20.057+0000] {processor.py:161} INFO - Started process (PID=2896) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T09:49:20.058+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T09:49:20.060+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:49:20.059+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:49:20.358+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:49:20.371+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:49:20.371+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T09:49:20.388+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:49:20.388+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-18 00:00:00+00:00, run_after=2024-03-18 01:00:00+00:00
[2024-03-19T09:49:20.402+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.349 seconds
[2024-03-19T09:49:51.019+0000] {processor.py:161} INFO - Started process (PID=2953) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T09:49:51.020+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T09:49:51.022+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:49:51.022+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:49:51.326+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:49:51.343+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:49:51.342+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T09:49:51.362+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:49:51.362+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-18 00:00:00+00:00, run_after=2024-03-18 01:00:00+00:00
[2024-03-19T09:49:51.379+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.364 seconds
[2024-03-19T09:50:21.791+0000] {processor.py:161} INFO - Started process (PID=3010) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T09:50:21.793+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T09:50:21.794+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:50:21.794+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:50:21.985+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:50:21.998+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:50:21.997+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T09:50:22.013+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:50:22.013+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-18 00:00:00+00:00, run_after=2024-03-18 01:00:00+00:00
[2024-03-19T09:50:22.027+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.238 seconds
[2024-03-19T09:50:52.082+0000] {processor.py:161} INFO - Started process (PID=3067) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T09:50:52.083+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T09:50:52.084+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:50:52.084+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:50:52.359+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:50:52.373+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:50:52.373+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T09:50:52.389+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:50:52.389+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-18 00:00:00+00:00, run_after=2024-03-18 01:00:00+00:00
[2024-03-19T09:50:52.403+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.324 seconds
[2024-03-19T09:51:33.340+0000] {processor.py:161} INFO - Started process (PID=3073) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T09:51:33.347+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T09:51:33.353+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:51:33.352+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:51:34.451+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:51:51.500+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:51:51.500+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T09:51:51.542+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:51:51.541+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-18 10:00:00+00:00, run_after=2024-03-18 11:00:00+00:00
[2024-03-19T09:51:51.578+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 18.252 seconds
[2024-03-19T09:52:40.964+0000] {processor.py:161} INFO - Started process (PID=3121) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T09:52:40.965+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T09:52:40.969+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:52:40.969+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:52:44.790+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:52:49.625+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:52:49.624+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-18T09:00:00+00:00', 'Hostname': 'd99d270e5c22', 'External Executor Id': '312ffbcb-bc26-49b8-891d-48305d605d35'}
[2024-03-19T09:52:49.693+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:52:49.692+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240318T090000, start_date=20240319T095224, end_date=20240319T095249
[2024-03-19T09:52:49.717+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-18T09:00:00+00:00 [failed]> in state failed
[2024-03-19T09:52:49.740+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:52:49.739+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T09:52:49.765+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:52:49.765+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-18 14:00:00+00:00, run_after=2024-03-18 15:00:00+00:00
[2024-03-19T09:52:49.787+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 8.828 seconds
[2024-03-19T09:52:50.979+0000] {processor.py:161} INFO - Started process (PID=3139) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T09:52:50.982+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T09:52:50.984+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:52:50.983+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:52:51.254+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:52:51.272+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:52:51.272+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-18T10:00:00+00:00', 'Hostname': 'd99d270e5c22', 'External Executor Id': '612751af-4e45-41d1-9ec9-54ffdc9de28f'}
[2024-03-19T09:52:51.297+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:52:51.297+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240318T100000, start_date=20240319T095241, end_date=20240319T095251
[2024-03-19T09:52:51.307+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-18T10:00:00+00:00 [failed]> in state failed
[2024-03-19T09:52:51.316+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:52:51.316+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T09:52:51.335+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:52:51.334+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-18 15:00:00+00:00, run_after=2024-03-18 16:00:00+00:00
[2024-03-19T09:52:51.351+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.375 seconds
[2024-03-19T09:53:25.130+0000] {processor.py:161} INFO - Started process (PID=3181) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T09:53:25.133+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T09:53:25.139+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:53:25.138+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:53:25.634+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:53:25.660+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:53:25.660+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T09:53:25.719+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.600 seconds
[2024-03-19T09:53:31.671+0000] {processor.py:161} INFO - Started process (PID=3192) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T09:53:31.672+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T09:53:31.674+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:53:31.674+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:53:32.012+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:53:32.037+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:53:32.037+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-18T16:00:00+00:00', 'Hostname': 'd99d270e5c22', 'External Executor Id': '88f50e2d-2531-4d2c-aec8-73ae450c9648'}
[2024-03-19T09:53:32.066+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:53:32.066+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240318T160000, start_date=20240319T095257, end_date=20240319T095332
[2024-03-19T09:53:32.078+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-18T16:00:00+00:00 [failed]> in state failed
[2024-03-19T09:53:32.082+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:53:32.082+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-18T17:00:00+00:00', 'Hostname': 'd99d270e5c22', 'External Executor Id': 'fd0ccd5a-7b41-45f0-b1b3-d0877ea04863'}
[2024-03-19T09:53:32.097+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:53:32.097+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240318T170000, start_date=20240319T095257, end_date=20240319T095332
[2024-03-19T09:53:32.102+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-18T17:00:00+00:00 [failed]> in state failed
[2024-03-19T09:53:32.105+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:53:32.105+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-18T20:00:00+00:00', 'Hostname': 'd99d270e5c22', 'External Executor Id': '60ce0b4e-0351-447a-b496-d57a7ae02c88'}
[2024-03-19T09:53:32.120+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:53:32.119+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240318T200000, start_date=20240319T095317, end_date=20240319T095332
[2024-03-19T09:53:32.130+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-18T20:00:00+00:00 [failed]> in state failed
[2024-03-19T09:53:32.141+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:53:32.141+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-18T21:00:00+00:00', 'Hostname': 'd99d270e5c22', 'External Executor Id': '72fbb9da-e02d-4f73-aeab-ddd26bc5be28'}
[2024-03-19T09:53:32.160+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:53:32.159+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240318T210000, start_date=20240319T095317, end_date=20240319T095332
[2024-03-19T09:53:32.168+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-18T21:00:00+00:00 [failed]> in state failed
[2024-03-19T09:53:32.176+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:53:32.176+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-19T01:00:00+00:00', 'Hostname': 'd99d270e5c22', 'External Executor Id': 'c017d6b9-5713-47be-b0ef-d7bbc6adca84'}
[2024-03-19T09:53:32.192+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:53:32.191+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240319T010000, start_date=20240319T095325, end_date=20240319T095332
[2024-03-19T09:53:32.196+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-19T01:00:00+00:00 [failed]> in state failed
[2024-03-19T09:53:32.200+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:53:32.199+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-18T22:00:00+00:00', 'Hostname': 'd99d270e5c22', 'External Executor Id': 'bf6d7848-6640-413e-80f7-563619982e92'}
[2024-03-19T09:53:32.214+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:53:32.214+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240318T220000, start_date=20240319T095317, end_date=20240319T095332
[2024-03-19T09:53:32.218+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-18T22:00:00+00:00 [failed]> in state failed
[2024-03-19T09:53:32.221+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:53:32.221+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-19T00:00:00+00:00', 'Hostname': 'd99d270e5c22', 'External Executor Id': '9638f206-e758-4afc-8ee1-4b886be1918c'}
[2024-03-19T09:53:32.234+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:53:32.234+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240319T000000, start_date=20240319T095325, end_date=20240319T095332
[2024-03-19T09:53:32.237+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-19T00:00:00+00:00 [failed]> in state failed
[2024-03-19T09:53:32.249+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:53:32.248+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T09:53:32.284+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.618 seconds
[2024-03-19T09:54:02.963+0000] {processor.py:161} INFO - Started process (PID=3251) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T09:54:02.964+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T09:54:02.966+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:54:02.966+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:54:03.194+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:54:03.208+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:54:03.208+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T09:54:03.227+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:54:03.227+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 09:00:00+00:00, run_after=2024-03-19 10:00:00+00:00
[2024-03-19T09:54:03.244+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.283 seconds
[2024-03-19T09:54:33.813+0000] {processor.py:161} INFO - Started process (PID=3306) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T09:54:33.814+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T09:54:33.815+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:54:33.815+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:54:34.089+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:54:34.103+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:54:34.103+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T09:54:34.119+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:54:34.119+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 09:00:00+00:00, run_after=2024-03-19 10:00:00+00:00
[2024-03-19T09:54:34.134+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.324 seconds
[2024-03-19T09:55:04.719+0000] {processor.py:161} INFO - Started process (PID=3363) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T09:55:04.720+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T09:55:04.722+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:55:04.721+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:55:05.053+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:55:05.068+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:55:05.068+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T09:55:05.087+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:55:05.087+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 09:00:00+00:00, run_after=2024-03-19 10:00:00+00:00
[2024-03-19T09:55:05.101+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.386 seconds
[2024-03-19T09:55:35.189+0000] {processor.py:161} INFO - Started process (PID=3420) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T09:55:35.190+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T09:55:35.192+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:55:35.191+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:55:35.568+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:55:35.587+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:55:35.587+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T09:55:35.612+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:55:35.612+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 09:00:00+00:00, run_after=2024-03-19 10:00:00+00:00
[2024-03-19T09:55:35.629+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.443 seconds
[2024-03-19T09:56:05.728+0000] {processor.py:161} INFO - Started process (PID=3477) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T09:56:05.730+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T09:56:05.731+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:56:05.731+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:56:06.034+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:56:06.053+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:56:06.053+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T09:56:06.072+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:56:06.071+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 09:00:00+00:00, run_after=2024-03-19 10:00:00+00:00
[2024-03-19T09:56:06.087+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.361 seconds
[2024-03-19T09:56:36.477+0000] {processor.py:161} INFO - Started process (PID=3534) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T09:56:36.479+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T09:56:36.479+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:56:36.479+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:56:36.814+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:56:36.833+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:56:36.832+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T09:56:36.857+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:56:36.856+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 09:00:00+00:00, run_after=2024-03-19 10:00:00+00:00
[2024-03-19T09:56:36.875+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.400 seconds
[2024-03-19T09:57:07.123+0000] {processor.py:161} INFO - Started process (PID=3593) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T09:57:07.124+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T09:57:07.125+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:57:07.125+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:57:07.341+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:57:07.356+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:57:07.356+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T09:57:07.373+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:57:07.373+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 09:00:00+00:00, run_after=2024-03-19 10:00:00+00:00
[2024-03-19T09:57:07.388+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.268 seconds
[2024-03-19T09:57:34.034+0000] {processor.py:161} INFO - Started process (PID=3645) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T09:57:34.035+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T09:57:34.036+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:57:34.035+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:57:34.332+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:57:34.352+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:57:34.352+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-18T07:00:00+00:00', 'Hostname': 'd99d270e5c22', 'External Executor Id': '9589debd-98e4-4e91-8809-81d417544f78'}
[2024-03-19T09:57:34.375+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:57:34.374+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240318T070000, start_date=20240319T095225, end_date=20240319T095734
[2024-03-19T09:57:34.382+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-18T07:00:00+00:00 [failed]> in state failed
[2024-03-19T09:57:34.394+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:57:34.394+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T09:57:34.413+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:57:34.413+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 09:00:00+00:00, run_after=2024-03-19 10:00:00+00:00
[2024-03-19T09:57:34.429+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.398 seconds
[2024-03-19T09:57:44.090+0000] {processor.py:161} INFO - Started process (PID=3688) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T09:57:44.091+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T09:57:44.092+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:57:44.092+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:57:44.318+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:57:44.336+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:57:44.336+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-18T03:00:00+00:00', 'Hostname': 'd99d270e5c22', 'External Executor Id': '70f30027-a04f-448d-be9e-16f2f6da6746'}
[2024-03-19T09:57:44.353+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:57:44.353+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240318T030000, start_date=20240319T095241, end_date=20240319T095744
[2024-03-19T09:57:44.361+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-18T03:00:00+00:00 [failed]> in state failed
[2024-03-19T09:57:44.369+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:57:44.369+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T09:57:44.384+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:57:44.384+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 09:00:00+00:00, run_after=2024-03-19 10:00:00+00:00
[2024-03-19T09:57:44.397+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.309 seconds
[2024-03-19T09:57:54.166+0000] {processor.py:161} INFO - Started process (PID=3696) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T09:57:54.167+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T09:57:54.168+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:57:54.167+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:57:54.240+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:57:54.267+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:57:54.267+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-18T05:00:00+00:00', 'Hostname': 'd99d270e5c22', 'External Executor Id': 'cd1cd8c5-6539-4ac0-93c7-b8ae53f5540d'}
[2024-03-19T09:57:54.292+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:57:54.292+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240318T050000, start_date=20240319T095241, end_date=20240319T095754
[2024-03-19T09:57:54.303+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-18T05:00:00+00:00 [failed]> in state failed
[2024-03-19T09:57:54.307+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:57:54.307+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-18T05:00:00+00:00', 'Hostname': 'd99d270e5c22', 'External Executor Id': 'cd1cd8c5-6539-4ac0-93c7-b8ae53f5540d'}
[2024-03-19T09:57:54.316+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:57:54.316+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240318T050000, start_date=20240319T095241, end_date=20240319T095754
[2024-03-19T09:57:54.320+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-18T05:00:00+00:00 [failed]> in state failed
[2024-03-19T09:57:54.330+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:57:54.330+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T09:57:54.348+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:57:54.347+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 09:00:00+00:00, run_after=2024-03-19 10:00:00+00:00
[2024-03-19T09:57:54.364+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.201 seconds
[2024-03-19T09:58:04.225+0000] {processor.py:161} INFO - Started process (PID=3704) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T09:58:04.227+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T09:58:04.227+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:58:04.227+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:58:04.295+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:58:04.318+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:58:04.318+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-18T18:00:00+00:00', 'Hostname': 'd99d270e5c22', 'External Executor Id': '13c7820d-e389-4807-8bc7-cf0db5441887'}
[2024-03-19T09:58:04.337+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:58:04.337+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240318T180000, start_date=20240319T095257, end_date=20240319T095804
[2024-03-19T09:58:04.345+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-18T18:00:00+00:00 [failed]> in state failed
[2024-03-19T09:58:04.358+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:58:04.357+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T09:58:04.375+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:58:04.375+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 09:00:00+00:00, run_after=2024-03-19 10:00:00+00:00
[2024-03-19T09:58:04.391+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.168 seconds
[2024-03-19T09:58:24.331+0000] {processor.py:161} INFO - Started process (PID=3754) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T09:58:24.332+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T09:58:24.333+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:58:24.332+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:58:24.401+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:58:24.425+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:58:24.424+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-18T19:00:00+00:00', 'Hostname': 'd99d270e5c22', 'External Executor Id': '785816fe-6fc9-4c50-801b-970d6050fbdb'}
[2024-03-19T09:58:24.444+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:58:24.443+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240318T190000, start_date=20240319T095317, end_date=20240319T095824
[2024-03-19T09:58:24.454+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-18T19:00:00+00:00 [failed]> in state failed
[2024-03-19T09:58:24.468+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:58:24.468+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T09:58:24.487+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:58:24.487+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 09:00:00+00:00, run_after=2024-03-19 10:00:00+00:00
[2024-03-19T09:58:24.503+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.175 seconds
[2024-03-19T09:58:34.387+0000] {processor.py:161} INFO - Started process (PID=3762) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T09:58:34.388+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T09:58:34.389+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:58:34.388+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:58:34.457+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:58:34.481+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:58:34.480+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-18T23:00:00+00:00', 'Hostname': 'd99d270e5c22', 'External Executor Id': '834a0176-ffdb-469a-8ed8-876c257915d2'}
[2024-03-19T09:58:34.500+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:58:34.500+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240318T230000, start_date=20240319T095325, end_date=20240319T095834
[2024-03-19T09:58:34.508+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-18T23:00:00+00:00 [failed]> in state failed
[2024-03-19T09:58:34.522+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:58:34.522+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T09:58:34.541+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:58:34.541+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 09:00:00+00:00, run_after=2024-03-19 10:00:00+00:00
[2024-03-19T09:58:34.560+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.176 seconds
[2024-03-19T09:59:05.142+0000] {processor.py:161} INFO - Started process (PID=3819) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T09:59:05.143+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T09:59:05.144+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:59:05.144+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:59:05.206+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:59:05.225+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:59:05.225+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T09:59:05.243+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:59:05.243+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 09:00:00+00:00, run_after=2024-03-19 10:00:00+00:00
[2024-03-19T09:59:05.258+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.119 seconds
[2024-03-19T09:59:35.591+0000] {processor.py:161} INFO - Started process (PID=3875) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T09:59:35.592+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T09:59:35.593+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:59:35.593+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:59:35.656+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T09:59:35.676+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:59:35.676+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T09:59:35.695+0000] {logging_mixin.py:188} INFO - [2024-03-19T09:59:35.695+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 09:00:00+00:00, run_after=2024-03-19 10:00:00+00:00
[2024-03-19T09:59:35.711+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.123 seconds
[2024-03-19T10:00:05.901+0000] {processor.py:161} INFO - Started process (PID=3932) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:00:05.902+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:00:05.903+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:00:05.903+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:00:05.973+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:00:05.991+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:00:05.991+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:00:06.013+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:00:06.013+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:00:06.029+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.131 seconds
[2024-03-19T10:00:36.224+0000] {processor.py:161} INFO - Started process (PID=3989) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:00:36.225+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:00:36.226+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:00:36.225+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:00:36.287+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:00:36.306+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:00:36.306+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:00:36.325+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:00:36.324+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:00:36.342+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.121 seconds
[2024-03-19T10:01:06.618+0000] {processor.py:161} INFO - Started process (PID=4046) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:01:06.619+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:01:06.620+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:01:06.620+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:01:06.684+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:01:06.703+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:01:06.702+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:01:06.722+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:01:06.722+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:01:06.736+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.121 seconds
[2024-03-19T10:01:37.017+0000] {processor.py:161} INFO - Started process (PID=4103) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:01:37.018+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:01:37.019+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:01:37.018+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:01:37.080+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:01:37.101+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:01:37.100+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:01:37.120+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:01:37.120+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:01:37.137+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.123 seconds
[2024-03-19T10:02:07.480+0000] {processor.py:161} INFO - Started process (PID=4160) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:02:07.481+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:02:07.482+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:02:07.482+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:02:07.545+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:02:07.563+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:02:07.563+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:02:07.583+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:02:07.583+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:02:07.596+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.119 seconds
[2024-03-19T10:02:38.013+0000] {processor.py:161} INFO - Started process (PID=4217) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:02:38.014+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:02:38.015+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:02:38.014+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:02:38.073+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:02:38.090+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:02:38.090+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:02:38.109+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:02:38.108+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:02:38.123+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.114 seconds
[2024-03-19T10:03:08.372+0000] {processor.py:161} INFO - Started process (PID=4274) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:03:08.373+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:03:08.374+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:03:08.374+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:03:08.431+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:03:08.447+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:03:08.447+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:03:08.467+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:03:08.467+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:03:08.484+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.114 seconds
[2024-03-19T10:03:38.699+0000] {processor.py:161} INFO - Started process (PID=4331) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:03:38.700+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:03:38.701+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:03:38.701+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:03:38.763+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:03:38.781+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:03:38.780+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:03:38.799+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:03:38.799+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:03:38.812+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.116 seconds
[2024-03-19T10:04:08.966+0000] {processor.py:161} INFO - Started process (PID=4388) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:04:08.967+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:04:08.968+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:04:08.967+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:04:09.044+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:04:09.061+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:04:09.061+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:04:09.081+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:04:09.081+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:04:09.100+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.137 seconds
[2024-03-19T10:04:39.481+0000] {processor.py:161} INFO - Started process (PID=4445) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:04:39.482+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:04:39.483+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:04:39.483+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:04:39.549+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:04:39.568+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:04:39.568+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:04:39.590+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:04:39.590+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:04:39.607+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.129 seconds
[2024-03-19T10:05:09.999+0000] {processor.py:161} INFO - Started process (PID=4502) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:05:10.000+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:05:10.001+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:05:10.001+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:05:10.081+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:05:10.098+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:05:10.097+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:05:10.116+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:05:10.116+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:05:10.134+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.138 seconds
[2024-03-19T10:05:40.475+0000] {processor.py:161} INFO - Started process (PID=4559) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:05:40.476+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:05:40.476+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:05:40.476+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:05:40.539+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:05:40.556+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:05:40.556+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:05:40.574+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:05:40.573+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:05:40.588+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.116 seconds
[2024-03-19T10:06:10.807+0000] {processor.py:161} INFO - Started process (PID=4616) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:06:10.808+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:06:10.808+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:06:10.808+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:06:10.889+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:06:10.908+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:06:10.907+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:06:10.926+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:06:10.926+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:06:10.942+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.138 seconds
[2024-03-19T10:06:41.212+0000] {processor.py:161} INFO - Started process (PID=4673) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:06:41.212+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:06:41.213+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:06:41.213+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:06:41.275+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:06:41.292+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:06:41.292+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:06:41.314+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:06:41.314+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:06:41.330+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.122 seconds
[2024-03-19T10:07:11.428+0000] {processor.py:161} INFO - Started process (PID=4730) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:07:11.429+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:07:11.430+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:07:11.430+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:07:11.491+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:07:11.509+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:07:11.509+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:07:11.528+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:07:11.528+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:07:11.546+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.121 seconds
[2024-03-19T10:07:41.682+0000] {processor.py:161} INFO - Started process (PID=4787) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:07:41.684+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:07:41.684+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:07:41.684+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:07:41.750+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:07:41.768+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:07:41.768+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:07:41.786+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:07:41.786+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:07:41.802+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.123 seconds
[2024-03-19T10:08:11.995+0000] {processor.py:161} INFO - Started process (PID=4843) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:08:11.997+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:08:11.999+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:08:11.998+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:08:12.070+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:08:12.092+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:08:12.092+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:08:12.119+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:08:12.119+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:08:12.140+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.148 seconds
[2024-03-19T10:08:42.355+0000] {processor.py:161} INFO - Started process (PID=4900) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:08:42.356+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:08:42.357+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:08:42.357+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:08:42.418+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:08:42.435+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:08:42.435+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:08:42.455+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:08:42.455+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:08:42.469+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.117 seconds
[2024-03-19T10:09:12.795+0000] {processor.py:161} INFO - Started process (PID=4957) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:09:12.796+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:09:12.797+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:09:12.797+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:09:12.863+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:09:12.880+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:09:12.880+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:09:12.899+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:09:12.899+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:09:12.916+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.123 seconds
[2024-03-19T10:09:43.309+0000] {processor.py:161} INFO - Started process (PID=5014) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:09:43.310+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:09:43.311+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:09:43.310+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:09:43.373+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:09:43.391+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:09:43.391+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:09:43.413+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:09:43.413+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:09:43.429+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.123 seconds
[2024-03-19T10:10:13.817+0000] {processor.py:161} INFO - Started process (PID=5071) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:10:13.819+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:10:13.820+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:10:13.819+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:10:13.883+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:10:13.903+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:10:13.902+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:10:13.921+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:10:13.921+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:10:13.935+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.120 seconds
[2024-03-19T10:10:44.312+0000] {processor.py:161} INFO - Started process (PID=5128) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:10:44.313+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:10:44.314+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:10:44.314+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:10:44.376+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:10:44.394+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:10:44.393+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:10:44.414+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:10:44.413+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:10:44.427+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.118 seconds
[2024-03-19T10:11:14.749+0000] {processor.py:161} INFO - Started process (PID=5185) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:11:14.750+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:11:14.751+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:11:14.751+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:11:14.811+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:11:14.829+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:11:14.828+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:11:14.849+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:11:14.848+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:11:14.867+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.122 seconds
[2024-03-19T10:11:45.202+0000] {processor.py:161} INFO - Started process (PID=5242) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:11:45.203+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:11:45.204+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:11:45.204+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:11:45.266+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:11:45.284+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:11:45.284+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:11:45.303+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:11:45.302+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:11:45.319+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.120 seconds
[2024-03-19T10:12:15.619+0000] {processor.py:161} INFO - Started process (PID=5299) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:12:15.619+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:12:15.620+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:12:15.620+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:12:15.683+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:12:15.702+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:12:15.702+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:12:15.720+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:12:15.720+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:12:15.735+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.119 seconds
[2024-03-19T10:12:46.174+0000] {processor.py:161} INFO - Started process (PID=5357) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:12:46.175+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:12:46.176+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:12:46.176+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:12:46.257+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:12:46.279+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:12:46.278+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:12:46.307+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:12:46.306+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:12:46.327+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.157 seconds
[2024-03-19T10:13:16.757+0000] {processor.py:161} INFO - Started process (PID=5414) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:13:16.758+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:13:16.759+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:13:16.759+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:13:16.824+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:13:16.843+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:13:16.843+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:13:16.867+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:13:16.867+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:13:16.882+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.128 seconds
[2024-03-19T10:13:47.407+0000] {processor.py:161} INFO - Started process (PID=5471) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:13:47.408+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:13:47.409+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:13:47.409+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:13:47.477+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:13:47.495+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:13:47.495+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:13:47.518+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:13:47.517+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:13:47.535+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.131 seconds
[2024-03-19T10:14:18.075+0000] {processor.py:161} INFO - Started process (PID=5529) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:14:18.076+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:14:18.077+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:14:18.077+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:14:18.166+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:14:18.187+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:14:18.187+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:14:18.211+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:14:18.211+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:14:18.233+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.163 seconds
[2024-03-19T10:14:48.759+0000] {processor.py:161} INFO - Started process (PID=5586) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:14:48.760+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:14:48.761+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:14:48.761+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:14:48.826+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:14:48.844+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:14:48.844+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:14:48.863+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:14:48.863+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:14:48.880+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.124 seconds
[2024-03-19T10:15:19.321+0000] {processor.py:161} INFO - Started process (PID=5643) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:15:19.322+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:15:19.323+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:15:19.323+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:15:19.389+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:15:19.409+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:15:19.409+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:15:19.430+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:15:19.429+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:15:19.443+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.126 seconds
[2024-03-19T10:15:49.855+0000] {processor.py:161} INFO - Started process (PID=5700) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:15:49.858+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:15:49.859+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:15:49.859+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:15:49.934+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:15:49.950+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:15:49.950+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:15:49.970+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:15:49.970+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:15:49.986+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.135 seconds
[2024-03-19T10:16:20.434+0000] {processor.py:161} INFO - Started process (PID=5757) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:16:20.436+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:16:20.437+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:16:20.436+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:16:20.541+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:16:20.568+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:16:20.568+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:16:20.602+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:16:20.601+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:16:20.632+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.202 seconds
[2024-03-19T10:16:50.984+0000] {processor.py:161} INFO - Started process (PID=5814) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:16:50.985+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:16:50.986+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:16:50.986+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:16:51.049+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:16:51.068+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:16:51.068+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:16:51.087+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:16:51.086+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:16:51.106+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.125 seconds
[2024-03-19T10:17:21.557+0000] {processor.py:161} INFO - Started process (PID=5871) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:17:21.558+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:17:21.559+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:17:21.559+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:17:21.622+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:17:21.639+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:17:21.639+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:17:21.661+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:17:21.660+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:17:21.674+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.120 seconds
[2024-03-19T10:17:51.975+0000] {processor.py:161} INFO - Started process (PID=5928) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:17:51.977+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:17:51.978+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:17:51.978+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:17:52.050+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:17:52.071+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:17:52.070+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:17:52.092+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:17:52.092+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:17:52.111+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.139 seconds
[2024-03-19T10:18:22.417+0000] {processor.py:161} INFO - Started process (PID=5985) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:18:22.418+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:18:22.420+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:18:22.419+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:18:22.494+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:18:22.515+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:18:22.514+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:18:22.537+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:18:22.537+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:18:22.560+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.147 seconds
[2024-03-19T10:18:52.874+0000] {processor.py:161} INFO - Started process (PID=6042) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:18:52.876+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:18:52.877+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:18:52.876+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:18:52.942+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:18:52.963+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:18:52.962+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:18:52.986+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:18:52.986+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:18:53.004+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.132 seconds
[2024-03-19T10:19:23.318+0000] {processor.py:161} INFO - Started process (PID=6099) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:19:23.319+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:19:23.320+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:19:23.319+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:19:23.389+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:19:23.410+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:19:23.410+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:19:23.429+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:19:23.429+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:19:23.446+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.131 seconds
[2024-03-19T10:19:53.922+0000] {processor.py:161} INFO - Started process (PID=6156) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:19:53.924+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:19:53.925+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:19:53.925+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:19:53.989+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:19:54.009+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:19:54.008+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:19:54.028+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:19:54.028+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:19:54.046+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.127 seconds
[2024-03-19T10:20:24.454+0000] {processor.py:161} INFO - Started process (PID=6213) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:20:24.455+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:20:24.456+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:20:24.455+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:20:24.518+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:20:24.536+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:20:24.536+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:20:24.556+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:20:24.555+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:20:24.574+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.123 seconds
[2024-03-19T10:20:55.012+0000] {processor.py:161} INFO - Started process (PID=6270) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:20:55.013+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:20:55.014+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:20:55.013+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:20:55.076+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:20:55.094+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:20:55.094+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:20:55.115+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:20:55.114+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:20:55.131+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.122 seconds
[2024-03-19T10:21:25.628+0000] {processor.py:161} INFO - Started process (PID=6327) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:21:25.629+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:21:25.630+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:21:25.630+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:21:25.701+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:21:25.723+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:21:25.723+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:21:25.745+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:21:25.745+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:21:25.765+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.140 seconds
[2024-03-19T10:21:56.001+0000] {processor.py:161} INFO - Started process (PID=6384) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:21:56.002+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:21:56.003+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:21:56.003+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:21:56.072+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:21:56.092+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:21:56.092+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:21:56.112+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:21:56.112+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:21:56.128+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.130 seconds
[2024-03-19T10:22:13.084+0000] {processor.py:161} INFO - Started process (PID=6434) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:22:13.085+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:22:13.086+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:22:13.086+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:22:13.154+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:22:13.234+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:22:13.233+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:22:13.254+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:22:13.254+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:22:13.275+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.193 seconds
[2024-03-19T10:22:29.615+0000] {processor.py:161} INFO - Started process (PID=6450) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:22:29.616+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:22:29.617+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:22:29.617+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:22:29.707+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:22:29.720+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:22:29.720+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:22:29.742+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:22:29.741+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:22:29.759+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.147 seconds
[2024-03-19T10:22:50.844+0000] {processor.py:161} INFO - Started process (PID=6498) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:22:50.845+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:22:50.845+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:22:50.845+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:22:50.905+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:22:50.973+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:22:50.972+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:22:50.989+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:22:50.989+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:22:51.007+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.165 seconds
[2024-03-19T10:22:55.920+0000] {processor.py:161} INFO - Started process (PID=6500) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:22:55.921+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:22:55.921+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:22:55.921+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:22:55.981+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:22:55.990+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:22:55.990+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:22:56.009+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:22:56.009+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:22:56.024+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.107 seconds
[2024-03-19T10:23:01.213+0000] {processor.py:161} INFO - Started process (PID=6512) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:23:01.214+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:23:01.215+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:23:01.215+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:23:01.297+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:23:01.310+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:23:01.310+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:23:01.332+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:23:01.332+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:23:01.350+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.140 seconds
[2024-03-19T10:23:31.788+0000] {processor.py:161} INFO - Started process (PID=6569) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:23:31.789+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:23:31.790+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:23:31.789+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:23:31.862+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:23:31.885+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:23:31.885+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:23:31.909+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:23:31.909+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:23:31.925+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.141 seconds
[2024-03-19T10:24:02.540+0000] {processor.py:161} INFO - Started process (PID=6626) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:24:02.541+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:24:02.542+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:24:02.541+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:24:02.624+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:24:02.645+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:24:02.645+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:24:02.670+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:24:02.670+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:24:02.688+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.151 seconds
[2024-03-19T10:24:32.946+0000] {processor.py:161} INFO - Started process (PID=6683) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:24:32.947+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:24:32.948+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:24:32.948+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:24:33.020+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:24:33.038+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:24:33.038+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:24:33.062+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:24:33.062+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:24:33.078+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.136 seconds
[2024-03-19T10:25:03.527+0000] {processor.py:161} INFO - Started process (PID=6740) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:25:03.528+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:25:03.529+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:25:03.529+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:25:03.606+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:25:03.624+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:25:03.624+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:25:03.644+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:25:03.644+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:25:03.664+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.140 seconds
[2024-03-19T10:25:34.218+0000] {processor.py:161} INFO - Started process (PID=6797) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:25:34.219+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:25:34.220+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:25:34.220+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:25:34.290+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:25:34.311+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:25:34.311+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:25:34.338+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:25:34.338+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:25:34.355+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.139 seconds
[2024-03-19T10:26:04.743+0000] {processor.py:161} INFO - Started process (PID=6854) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:26:04.745+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:26:04.746+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:26:04.746+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:26:04.828+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:26:04.855+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:26:04.854+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:26:04.883+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:26:04.883+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:26:04.908+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.169 seconds
[2024-03-19T10:26:23.969+0000] {processor.py:161} INFO - Started process (PID=6899) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:26:23.970+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:26:23.971+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:26:23.970+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:26:24.038+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:26:24.035+0000] {dagbag.py:447} ERROR - Failed to bag_dag: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 100, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 813, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 659, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 442, in _process_modules
    dag.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 740, in validate
    self.timetable.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 102, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2024-03-19T10:26:24.039+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:26:24.057+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.091 seconds
[2024-03-19T10:26:27.040+0000] {processor.py:161} INFO - Started process (PID=6900) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:26:27.041+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:26:27.042+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:26:27.042+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:26:27.103+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:26:27.102+0000] {dagbag.py:447} ERROR - Failed to bag_dag: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 100, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 813, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 659, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 442, in _process_modules
    dag.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 740, in validate
    self.timetable.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 102, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2024-03-19T10:26:27.103+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:26:27.118+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.081 seconds
[2024-03-19T10:26:57.600+0000] {processor.py:161} INFO - Started process (PID=6957) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:26:57.601+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:26:57.602+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:26:57.602+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:26:57.673+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:26:57.672+0000] {dagbag.py:447} ERROR - Failed to bag_dag: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 100, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 813, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 659, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 442, in _process_modules
    dag.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 740, in validate
    self.timetable.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 102, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2024-03-19T10:26:57.674+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:26:57.692+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.096 seconds
[2024-03-19T10:27:10.575+0000] {processor.py:161} INFO - Started process (PID=7006) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:27:10.576+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:27:10.577+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:27:10.576+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:27:10.635+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:27:10.634+0000] {dagbag.py:447} ERROR - Failed to bag_dag: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 100, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 813, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 659, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 442, in _process_modules
    dag.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 740, in validate
    self.timetable.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 102, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2024-03-19T10:27:10.636+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:27:10.652+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.080 seconds
[2024-03-19T10:27:40.739+0000] {processor.py:161} INFO - Started process (PID=7056) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:27:40.740+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:27:40.741+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:27:40.741+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:27:40.803+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:27:40.802+0000] {dagbag.py:447} ERROR - Failed to bag_dag: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 100, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 813, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 659, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 442, in _process_modules
    dag.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 740, in validate
    self.timetable.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 102, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2024-03-19T10:27:40.803+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:27:40.819+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.083 seconds
[2024-03-19T10:28:11.134+0000] {processor.py:161} INFO - Started process (PID=7114) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:28:11.135+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:28:11.136+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:28:11.136+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:28:11.199+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:28:11.198+0000] {dagbag.py:447} ERROR - Failed to bag_dag: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 100, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 813, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 659, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 442, in _process_modules
    dag.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 740, in validate
    self.timetable.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 102, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2024-03-19T10:28:11.200+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:28:11.215+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.083 seconds
[2024-03-19T10:28:41.615+0000] {processor.py:161} INFO - Started process (PID=7171) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:28:41.615+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:28:41.616+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:28:41.616+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:28:41.672+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:28:41.671+0000] {dagbag.py:447} ERROR - Failed to bag_dag: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 100, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 813, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 659, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 442, in _process_modules
    dag.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 740, in validate
    self.timetable.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 102, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2024-03-19T10:28:41.673+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:28:41.688+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.076 seconds
[2024-03-19T10:29:12.001+0000] {processor.py:161} INFO - Started process (PID=7228) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:29:12.002+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:29:12.002+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:29:12.002+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:29:12.062+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:29:12.061+0000] {dagbag.py:447} ERROR - Failed to bag_dag: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 100, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 813, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 659, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 442, in _process_modules
    dag.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 740, in validate
    self.timetable.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 102, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2024-03-19T10:29:12.062+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:29:12.076+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.078 seconds
[2024-03-19T10:29:42.386+0000] {processor.py:161} INFO - Started process (PID=7283) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:29:42.387+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:29:42.387+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:29:42.387+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:29:42.447+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:29:42.446+0000] {dagbag.py:447} ERROR - Failed to bag_dag: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 100, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 813, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 659, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 442, in _process_modules
    dag.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 740, in validate
    self.timetable.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 102, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2024-03-19T10:29:42.448+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:29:42.462+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.079 seconds
[2024-03-19T10:30:12.546+0000] {processor.py:161} INFO - Started process (PID=7337) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:30:12.547+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:30:12.547+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:30:12.547+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:30:12.610+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:30:12.609+0000] {dagbag.py:447} ERROR - Failed to bag_dag: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 100, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 813, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 659, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 442, in _process_modules
    dag.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 740, in validate
    self.timetable.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 102, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2024-03-19T10:30:12.611+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:30:12.623+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.080 seconds
[2024-03-19T10:30:42.739+0000] {processor.py:161} INFO - Started process (PID=7391) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:30:42.740+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:30:42.740+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:30:42.740+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:30:42.805+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:30:42.804+0000] {dagbag.py:447} ERROR - Failed to bag_dag: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 100, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 813, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 659, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 442, in _process_modules
    dag.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 740, in validate
    self.timetable.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 102, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2024-03-19T10:30:42.805+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:30:42.822+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.086 seconds
[2024-03-19T10:31:12.884+0000] {processor.py:161} INFO - Started process (PID=7443) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:31:12.885+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:31:12.886+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:31:12.885+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:31:12.947+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:31:12.945+0000] {dagbag.py:447} ERROR - Failed to bag_dag: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 100, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 813, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 659, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 442, in _process_modules
    dag.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 740, in validate
    self.timetable.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 102, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2024-03-19T10:31:12.947+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:31:12.962+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.081 seconds
[2024-03-19T10:31:43.276+0000] {processor.py:161} INFO - Started process (PID=7500) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:31:43.277+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:31:43.278+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:31:43.278+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:31:43.346+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:31:43.345+0000] {dagbag.py:447} ERROR - Failed to bag_dag: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 100, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 813, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 659, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 442, in _process_modules
    dag.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 740, in validate
    self.timetable.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 102, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2024-03-19T10:31:43.347+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:31:43.365+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.091 seconds
[2024-03-19T10:32:13.779+0000] {processor.py:161} INFO - Started process (PID=7557) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:32:13.781+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:32:13.782+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:32:13.782+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:32:13.857+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:32:13.856+0000] {dagbag.py:447} ERROR - Failed to bag_dag: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 100, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 813, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 659, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 442, in _process_modules
    dag.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 740, in validate
    self.timetable.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 102, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2024-03-19T10:32:13.858+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:32:13.875+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.099 seconds
[2024-03-19T10:32:44.300+0000] {processor.py:161} INFO - Started process (PID=7614) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:32:44.301+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:32:44.302+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:32:44.301+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:32:44.374+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:32:44.372+0000] {dagbag.py:447} ERROR - Failed to bag_dag: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 100, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 813, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 659, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 442, in _process_modules
    dag.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 740, in validate
    self.timetable.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 102, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2024-03-19T10:32:44.374+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:32:44.391+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.095 seconds
[2024-03-19T10:33:14.694+0000] {processor.py:161} INFO - Started process (PID=7671) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:33:14.695+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:33:14.696+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:33:14.696+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:33:14.770+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:33:14.768+0000] {dagbag.py:447} ERROR - Failed to bag_dag: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 100, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 813, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 659, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 442, in _process_modules
    dag.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 740, in validate
    self.timetable.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 102, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2024-03-19T10:33:14.771+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:33:14.791+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.100 seconds
[2024-03-19T10:33:45.051+0000] {processor.py:161} INFO - Started process (PID=7728) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:33:45.052+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:33:45.053+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:33:45.052+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:33:45.121+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:33:45.118+0000] {dagbag.py:447} ERROR - Failed to bag_dag: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 100, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 813, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 659, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 442, in _process_modules
    dag.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 740, in validate
    self.timetable.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 102, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2024-03-19T10:33:45.122+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:33:45.138+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.090 seconds
[2024-03-19T10:34:15.392+0000] {processor.py:161} INFO - Started process (PID=7785) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:34:15.393+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:34:15.394+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:34:15.394+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:34:15.467+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:34:15.466+0000] {dagbag.py:447} ERROR - Failed to bag_dag: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 100, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 813, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 659, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 442, in _process_modules
    dag.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 740, in validate
    self.timetable.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 102, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2024-03-19T10:34:15.468+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:34:15.485+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.097 seconds
[2024-03-19T10:34:45.680+0000] {processor.py:161} INFO - Started process (PID=7842) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:34:45.681+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:34:45.682+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:34:45.682+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:34:45.753+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:34:45.751+0000] {dagbag.py:447} ERROR - Failed to bag_dag: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 100, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 813, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 659, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 442, in _process_modules
    dag.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 740, in validate
    self.timetable.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 102, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2024-03-19T10:34:45.754+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:34:45.774+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.097 seconds
[2024-03-19T10:35:15.961+0000] {processor.py:161} INFO - Started process (PID=7899) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:35:15.962+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:35:15.963+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:35:15.963+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:35:16.033+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:35:16.032+0000] {dagbag.py:447} ERROR - Failed to bag_dag: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 100, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 813, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 659, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 442, in _process_modules
    dag.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 740, in validate
    self.timetable.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 102, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2024-03-19T10:35:16.034+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:35:16.050+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.092 seconds
[2024-03-19T10:35:46.332+0000] {processor.py:161} INFO - Started process (PID=7956) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:35:46.333+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:35:46.334+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:35:46.334+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:35:46.401+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:35:46.399+0000] {dagbag.py:447} ERROR - Failed to bag_dag: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 100, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 813, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 659, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 442, in _process_modules
    dag.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 740, in validate
    self.timetable.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 102, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2024-03-19T10:35:46.401+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:35:46.419+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.090 seconds
[2024-03-19T10:36:16.531+0000] {processor.py:161} INFO - Started process (PID=8013) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:36:16.532+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:36:16.533+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:36:16.532+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:36:16.604+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:36:16.602+0000] {dagbag.py:447} ERROR - Failed to bag_dag: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 100, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 813, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 659, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 442, in _process_modules
    dag.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 740, in validate
    self.timetable.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 102, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2024-03-19T10:36:16.604+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:36:16.620+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.093 seconds
[2024-03-19T10:36:46.969+0000] {processor.py:161} INFO - Started process (PID=8070) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:36:46.970+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:36:46.971+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:36:46.970+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:36:47.040+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:36:47.039+0000] {dagbag.py:447} ERROR - Failed to bag_dag: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 100, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 813, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 659, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 442, in _process_modules
    dag.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 740, in validate
    self.timetable.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 102, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2024-03-19T10:36:47.041+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:36:47.058+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.093 seconds
[2024-03-19T10:37:17.566+0000] {processor.py:161} INFO - Started process (PID=8126) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:37:17.567+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:37:17.568+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:37:17.568+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:37:17.639+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:37:17.637+0000] {dagbag.py:447} ERROR - Failed to bag_dag: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 100, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 813, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 659, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 442, in _process_modules
    dag.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 740, in validate
    self.timetable.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 102, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2024-03-19T10:37:17.639+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:37:17.657+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.094 seconds
[2024-03-19T10:37:47.985+0000] {processor.py:161} INFO - Started process (PID=8183) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:37:47.986+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:37:47.987+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:37:47.987+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:37:48.058+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:37:48.054+0000] {dagbag.py:447} ERROR - Failed to bag_dag: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 100, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 813, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 659, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 442, in _process_modules
    dag.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 740, in validate
    self.timetable.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 102, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2024-03-19T10:37:48.058+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:37:48.076+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.094 seconds
[2024-03-19T10:38:18.527+0000] {processor.py:161} INFO - Started process (PID=8240) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:38:18.528+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:38:18.529+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:38:18.529+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:38:18.615+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:38:18.613+0000] {dagbag.py:447} ERROR - Failed to bag_dag: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 100, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 813, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 659, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 442, in _process_modules
    dag.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 740, in validate
    self.timetable.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 102, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2024-03-19T10:38:18.615+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:38:18.632+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.109 seconds
[2024-03-19T10:38:19.411+0000] {processor.py:161} INFO - Started process (PID=8261) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:38:19.412+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:38:19.413+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:38:19.412+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:38:19.479+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:38:19.477+0000] {dagbag.py:447} ERROR - Failed to bag_dag: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 100, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 813, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 659, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 442, in _process_modules
    dag.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 740, in validate
    self.timetable.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 102, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2024-03-19T10:38:19.479+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:38:19.495+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.087 seconds
[2024-03-19T10:38:49.836+0000] {processor.py:161} INFO - Started process (PID=8316) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:38:49.837+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:38:49.838+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:38:49.838+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:38:49.898+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:38:49.897+0000] {dagbag.py:447} ERROR - Failed to bag_dag: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 100, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 813, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 659, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 442, in _process_modules
    dag.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 740, in validate
    self.timetable.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 102, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2024-03-19T10:38:49.899+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:38:49.915+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.081 seconds
[2024-03-19T10:39:20.020+0000] {processor.py:161} INFO - Started process (PID=8363) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:39:20.021+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:39:20.022+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:39:20.022+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:39:20.099+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:39:20.098+0000] {dagbag.py:447} ERROR - Failed to bag_dag: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 100, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 813, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 659, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 442, in _process_modules
    dag.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 740, in validate
    self.timetable.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 102, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2024-03-19T10:39:20.100+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:39:20.119+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.102 seconds
[2024-03-19T10:39:50.162+0000] {processor.py:161} INFO - Started process (PID=8412) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:39:50.163+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:39:50.164+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:39:50.164+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:39:50.232+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:39:50.231+0000] {dagbag.py:447} ERROR - Failed to bag_dag: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 100, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 813, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 659, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 442, in _process_modules
    dag.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 740, in validate
    self.timetable.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 102, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2024-03-19T10:39:50.233+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:39:50.251+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.093 seconds
[2024-03-19T10:40:20.428+0000] {processor.py:161} INFO - Started process (PID=8469) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:40:20.429+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:40:20.429+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:40:20.429+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:40:20.504+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:40:20.503+0000] {dagbag.py:447} ERROR - Failed to bag_dag: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 100, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 813, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 659, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 442, in _process_modules
    dag.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 740, in validate
    self.timetable.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 102, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2024-03-19T10:40:20.504+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:40:20.525+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.101 seconds
[2024-03-19T10:40:25.876+0000] {processor.py:161} INFO - Started process (PID=8492) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:40:25.876+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:40:25.877+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:40:25.877+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:40:25.937+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:40:25.936+0000] {dagbag.py:447} ERROR - Failed to bag_dag: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 100, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 813, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 659, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 442, in _process_modules
    dag.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 740, in validate
    self.timetable.validate()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 102, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2024-03-19T10:40:25.938+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:40:25.952+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.080 seconds
[2024-03-19T10:40:31.112+0000] {processor.py:161} INFO - Started process (PID=8502) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:40:31.113+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:40:31.114+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:40:31.114+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:40:31.185+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:40:31.261+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:40:31.261+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:40:31.280+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:40:31.279+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:00:00+00:00, run_after=2024-03-19 10:01:00+00:00
[2024-03-19T10:40:31.302+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.194 seconds
[2024-03-19T10:41:09.419+0000] {processor.py:161} INFO - Started process (PID=8532) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:41:09.881+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:41:10.587+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:41:10.126+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:41:25.627+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:41:25.664+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:41:25.663+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:41:25.701+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:41:25.700+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:12:00+00:00, run_after=2024-03-19 10:13:00+00:00
[2024-03-19T10:41:25.736+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 16.797 seconds
[2024-03-19T10:41:28.773+0000] {processor.py:161} INFO - Started process (PID=8563) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:41:28.775+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:41:28.780+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:41:28.779+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:41:28.957+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:41:28.987+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:41:28.987+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:41:29.024+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:41:29.024+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:21:00+00:00, run_after=2024-03-19 10:22:00+00:00
[2024-03-19T10:41:29.076+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.307 seconds
[2024-03-19T10:42:16.075+0000] {processor.py:161} INFO - Started process (PID=8576) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:42:16.078+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:42:16.080+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:42:16.079+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:42:16.310+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:42:16.358+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:42:16.358+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:42:16.399+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:42:16.397+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:24:00+00:00, run_after=2024-03-19 10:25:00+00:00
[2024-03-19T10:42:16.439+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.371 seconds
[2024-03-19T10:42:22.123+0000] {processor.py:161} INFO - Started process (PID=8588) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:42:22.124+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:42:22.126+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:42:22.125+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:42:22.288+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:42:22.318+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:42:22.317+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-19T10:13:00+00:00', 'Hostname': 'd99d270e5c22', 'External Executor Id': 'a1864348-6f64-4a45-9b25-e62d5a46c45e'}
[2024-03-19T10:42:22.351+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:42:22.351+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240319T101300, start_date=20240319T104129, end_date=20240319T104222
[2024-03-19T10:42:22.372+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-19T10:13:00+00:00 [failed]> in state failed
[2024-03-19T10:42:22.395+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:42:22.395+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:42:22.428+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:42:22.427+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:24:00+00:00, run_after=2024-03-19 10:25:00+00:00
[2024-03-19T10:42:22.461+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.342 seconds
[2024-03-19T10:42:29.577+0000] {processor.py:161} INFO - Started process (PID=8604) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:42:29.578+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:42:29.580+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:42:29.580+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:42:29.755+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:42:29.779+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:42:29.779+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:42:29.804+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:42:29.804+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:25:00+00:00, run_after=2024-03-19 10:26:00+00:00
[2024-03-19T10:42:29.825+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.253 seconds
[2024-03-19T10:42:32.395+0000] {processor.py:161} INFO - Started process (PID=8631) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:42:32.396+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:42:32.398+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:42:32.397+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:42:32.519+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:42:32.543+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:42:32.543+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:42:32.567+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:42:32.566+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:29:00+00:00, run_after=2024-03-19 10:30:00+00:00
[2024-03-19T10:42:32.586+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.195 seconds
[2024-03-19T10:42:56.510+0000] {processor.py:161} INFO - Started process (PID=8644) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:42:56.511+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:42:56.513+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:42:56.512+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:42:56.649+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:42:56.678+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:42:56.678+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-19T10:22:00+00:00', 'Hostname': 'd99d270e5c22', 'External Executor Id': 'c0cfe073-f321-4001-b292-562938ebee4d'}
[2024-03-19T10:42:56.707+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:42:56.706+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240319T102200, start_date=20240319T104223, end_date=20240319T104256
[2024-03-19T10:42:56.717+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-19T10:22:00+00:00 [failed]> in state failed
[2024-03-19T10:42:56.720+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:42:56.720+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-19T10:23:00+00:00', 'Hostname': 'd99d270e5c22', 'External Executor Id': 'd78b148f-31b3-45a0-a727-cc2935830cac'}
[2024-03-19T10:42:56.731+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:42:56.730+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240319T102300, start_date=20240319T104223, end_date=20240319T104256
[2024-03-19T10:42:56.734+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-19T10:23:00+00:00 [failed]> in state failed
[2024-03-19T10:42:56.737+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:42:56.737+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-19T10:24:00+00:00', 'Hostname': 'd99d270e5c22', 'External Executor Id': '613acf24-3c5d-43c4-b286-dd1214fa0dbe'}
[2024-03-19T10:42:56.748+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:42:56.748+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240319T102400, start_date=20240319T104232, end_date=20240319T104256
[2024-03-19T10:42:56.751+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-19T10:24:00+00:00 [failed]> in state failed
[2024-03-19T10:42:56.761+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:42:56.760+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:42:56.779+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:42:56.778+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:31:00+00:00, run_after=2024-03-19 10:32:00+00:00
[2024-03-19T10:42:56.797+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.291 seconds
[2024-03-19T10:43:16.158+0000] {processor.py:161} INFO - Started process (PID=8659) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:43:16.159+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:43:16.161+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:16.161+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:43:16.316+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:43:16.339+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:16.338+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:43:16.369+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:16.369+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:32:00+00:00, run_after=2024-03-19 10:33:00+00:00
[2024-03-19T10:43:16.394+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.241 seconds
[2024-03-19T10:43:28.345+0000] {processor.py:161} INFO - Started process (PID=8692) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:43:28.346+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:43:28.348+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:28.347+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:43:28.548+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:43:28.599+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:28.598+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-19T10:26:00+00:00', 'Hostname': 'd99d270e5c22', 'External Executor Id': 'dafef1cc-220c-4190-b7e9-1e4d12d089a5'}
[2024-03-19T10:43:28.639+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:28.639+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240319T102600, start_date=20240319T104240, end_date=20240319T104328
[2024-03-19T10:43:28.657+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-19T10:26:00+00:00 [failed]> in state failed
[2024-03-19T10:43:28.662+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:28.662+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-19T10:25:00+00:00', 'Hostname': 'd99d270e5c22', 'External Executor Id': '62b5896a-b31f-45bb-88be-78746e8e2d13'}
[2024-03-19T10:43:28.683+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:28.683+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240319T102500, start_date=20240319T104238, end_date=20240319T104328
[2024-03-19T10:43:28.690+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-19T10:25:00+00:00 [failed]> in state failed
[2024-03-19T10:43:28.706+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:28.706+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:43:28.734+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:28.734+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:38:00+00:00, run_after=2024-03-19 10:39:00+00:00
[2024-03-19T10:43:28.761+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.421 seconds
[2024-03-19T10:43:28.803+0000] {processor.py:161} INFO - Started process (PID=8697) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:43:28.804+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:43:28.807+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:28.806+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:43:28.946+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:43:28.987+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:28.985+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-19T10:26:00+00:00', 'Hostname': 'd99d270e5c22', 'External Executor Id': 'dafef1cc-220c-4190-b7e9-1e4d12d089a5'}
[2024-03-19T10:43:29.020+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:29.020+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240319T102600, start_date=20240319T104240, end_date=20240319T104328
[2024-03-19T10:43:29.032+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-19T10:26:00+00:00 [failed]> in state failed
[2024-03-19T10:43:29.038+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:29.038+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-19T10:28:00+00:00', 'Hostname': 'd99d270e5c22', 'External Executor Id': '1199a9b0-39d1-4291-a51f-5193f000a122'}
[2024-03-19T10:43:29.052+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:29.052+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240319T102800, start_date=20240319T104256, end_date=20240319T104329
[2024-03-19T10:43:29.056+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-19T10:28:00+00:00 [failed]> in state failed
[2024-03-19T10:43:29.060+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:29.059+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-19T10:27:00+00:00', 'Hostname': 'd99d270e5c22', 'External Executor Id': '0f000b0d-64af-4b8f-a8c2-22e121fd164b'}
[2024-03-19T10:43:29.072+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:29.072+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240319T102700, start_date=20240319T104256, end_date=20240319T104329
[2024-03-19T10:43:29.076+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-19T10:27:00+00:00 [failed]> in state failed
[2024-03-19T10:43:29.080+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:29.080+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-19T10:30:00+00:00', 'Hostname': 'd99d270e5c22', 'External Executor Id': 'f09f3b5b-e36f-4862-8c3d-235a9701782d'}
[2024-03-19T10:43:29.093+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:29.092+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240319T103000, start_date=20240319T104316, end_date=20240319T104329
[2024-03-19T10:43:29.097+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-19T10:30:00+00:00 [failed]> in state failed
[2024-03-19T10:43:29.111+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:29.110+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:43:29.137+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:29.137+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:38:00+00:00, run_after=2024-03-19 10:39:00+00:00
[2024-03-19T10:43:29.166+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.368 seconds
[2024-03-19T10:43:44.202+0000] {processor.py:161} INFO - Started process (PID=8709) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:43:44.204+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:43:44.205+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:44.205+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:43:44.314+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:43:44.339+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:44.339+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-19T10:31:00+00:00', 'Hostname': 'd99d270e5c22', 'External Executor Id': '6e7ff9e7-d8bb-42f1-96a2-88b90e39d589'}
[2024-03-19T10:43:44.361+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:44.361+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240319T103100, start_date=20240319T104319, end_date=20240319T104344
[2024-03-19T10:43:44.369+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-19T10:31:00+00:00 [failed]> in state failed
[2024-03-19T10:43:44.372+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:44.372+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-19T10:32:00+00:00', 'Hostname': 'd99d270e5c22', 'External Executor Id': '563849db-c113-47f3-a236-1e6d76a3c9b5'}
[2024-03-19T10:43:44.382+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:44.382+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240319T103200, start_date=20240319T104329, end_date=20240319T104344
[2024-03-19T10:43:44.385+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-19T10:32:00+00:00 [failed]> in state failed
[2024-03-19T10:43:44.397+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:44.397+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:43:44.416+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:44.415+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:39:00+00:00, run_after=2024-03-19 10:40:00+00:00
[2024-03-19T10:43:44.434+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.236 seconds
[2024-03-19T10:43:54.916+0000] {processor.py:161} INFO - Started process (PID=8741) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:43:54.917+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:43:54.918+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:54.917+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:43:55.020+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:43:55.042+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:55.042+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-19T10:34:00+00:00', 'Hostname': 'd99d270e5c22', 'External Executor Id': '84b93fac-4c8c-4ef8-9dcc-19db45aa9dc6'}
[2024-03-19T10:43:55.066+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:55.065+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240319T103400, start_date=20240319T104329, end_date=20240319T104355
[2024-03-19T10:43:55.073+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-19T10:34:00+00:00 [failed]> in state failed
[2024-03-19T10:43:55.076+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:55.075+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-19T10:36:00+00:00', 'Hostname': 'd99d270e5c22', 'External Executor Id': 'f815fcae-dcb4-4075-b53c-5eddb0039999'}
[2024-03-19T10:43:55.086+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:55.086+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240319T103600, start_date=20240319T104344, end_date=20240319T104355
[2024-03-19T10:43:55.089+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-19T10:36:00+00:00 [failed]> in state failed
[2024-03-19T10:43:55.101+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:55.101+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:43:55.123+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:43:55.122+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:40:00+00:00, run_after=2024-03-19 10:41:00+00:00
[2024-03-19T10:43:55.145+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.233 seconds
[2024-03-19T10:44:25.252+0000] {processor.py:161} INFO - Started process (PID=8798) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:44:25.254+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:44:25.255+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:44:25.255+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:44:25.341+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:44:25.360+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:44:25.360+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:44:25.383+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:44:25.383+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:44:00+00:00, run_after=2024-03-19 10:45:00+00:00
[2024-03-19T10:44:25.402+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.153 seconds
[2024-03-19T10:44:55.589+0000] {processor.py:161} INFO - Started process (PID=8855) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:44:55.590+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:44:55.591+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:44:55.591+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:44:55.652+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:44:55.667+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:44:55.667+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:44:55.686+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:44:55.686+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:44:00+00:00, run_after=2024-03-19 10:45:00+00:00
[2024-03-19T10:44:55.700+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.114 seconds
[2024-03-19T10:45:25.729+0000] {processor.py:161} INFO - Started process (PID=8912) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:45:25.730+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:45:25.732+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:45:25.732+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:45:25.794+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:45:25.810+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:45:25.810+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:45:25.827+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:45:25.826+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:45:00+00:00, run_after=2024-03-19 10:46:00+00:00
[2024-03-19T10:45:25.839+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.112 seconds
[2024-03-19T10:45:56.519+0000] {processor.py:161} INFO - Started process (PID=8968) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:45:56.520+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:45:56.521+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:45:56.520+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:45:56.586+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:45:56.604+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:45:56.604+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:45:56.630+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:45:56.630+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:45:00+00:00, run_after=2024-03-19 10:46:00+00:00
[2024-03-19T10:45:56.648+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.131 seconds
[2024-03-19T10:46:26.922+0000] {processor.py:161} INFO - Started process (PID=9020) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:46:26.923+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:46:26.924+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:46:26.924+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:46:26.990+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:46:27.007+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:46:27.007+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:46:27.029+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:46:27.028+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:46:00+00:00, run_after=2024-03-19 10:47:00+00:00
[2024-03-19T10:46:27.042+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.123 seconds
[2024-03-19T10:46:57.341+0000] {processor.py:161} INFO - Started process (PID=9076) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:46:57.342+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:46:57.343+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:46:57.343+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:46:57.420+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:46:57.436+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:46:57.436+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:46:57.459+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:46:57.459+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:46:00+00:00, run_after=2024-03-19 10:47:00+00:00
[2024-03-19T10:46:57.474+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.136 seconds
[2024-03-19T10:47:27.568+0000] {processor.py:161} INFO - Started process (PID=9133) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:47:27.569+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:47:27.570+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:47:27.570+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:47:27.641+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:47:27.658+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:47:27.657+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:47:27.677+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:47:27.677+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:47:00+00:00, run_after=2024-03-19 10:48:00+00:00
[2024-03-19T10:47:27.697+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.132 seconds
[2024-03-19T10:47:57.759+0000] {processor.py:161} INFO - Started process (PID=9181) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:47:57.760+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:47:57.762+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:47:57.762+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:47:57.844+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:47:57.865+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:47:57.864+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:47:57.886+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:47:57.886+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:47:00+00:00, run_after=2024-03-19 10:48:00+00:00
[2024-03-19T10:47:57.907+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.152 seconds
[2024-03-19T10:48:28.146+0000] {processor.py:161} INFO - Started process (PID=9238) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:48:28.147+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:48:28.148+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:48:28.148+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:48:28.213+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:48:28.229+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:48:28.229+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:48:28.251+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:48:28.251+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:48:00+00:00, run_after=2024-03-19 10:49:00+00:00
[2024-03-19T10:48:28.267+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.124 seconds
[2024-03-19T10:48:58.852+0000] {processor.py:161} INFO - Started process (PID=9295) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:48:58.854+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:48:58.856+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:48:58.855+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:48:58.939+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:48:58.955+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:48:58.955+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:48:58.976+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:48:58.976+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:48:00+00:00, run_after=2024-03-19 10:49:00+00:00
[2024-03-19T10:48:58.991+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.144 seconds
[2024-03-19T10:49:29.321+0000] {processor.py:161} INFO - Started process (PID=9352) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:49:29.322+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:49:29.323+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:49:29.323+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:49:29.395+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:49:29.410+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:49:29.410+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:49:29.434+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:49:29.433+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:49:00+00:00, run_after=2024-03-19 10:50:00+00:00
[2024-03-19T10:49:29.450+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.132 seconds
[2024-03-19T10:49:59.711+0000] {processor.py:161} INFO - Started process (PID=9416) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:49:59.713+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:49:59.713+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:49:59.713+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:49:59.788+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:49:59.805+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:49:59.805+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:49:59.827+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:49:59.827+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:49:00+00:00, run_after=2024-03-19 10:50:00+00:00
[2024-03-19T10:49:59.846+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.137 seconds
[2024-03-19T10:50:29.924+0000] {processor.py:161} INFO - Started process (PID=9473) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:50:29.925+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:50:29.926+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:50:29.926+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:50:29.996+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:50:30.015+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:50:30.015+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:50:30.037+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:50:30.036+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:50:00+00:00, run_after=2024-03-19 10:51:00+00:00
[2024-03-19T10:50:30.053+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.132 seconds
[2024-03-19T10:51:00.266+0000] {processor.py:161} INFO - Started process (PID=9530) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:51:00.267+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:51:00.268+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:51:00.267+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:51:00.336+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:51:00.354+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:51:00.354+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:51:00.374+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:51:00.374+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:50:00+00:00, run_after=2024-03-19 10:51:00+00:00
[2024-03-19T10:51:00.389+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.126 seconds
[2024-03-19T10:51:30.795+0000] {processor.py:161} INFO - Started process (PID=9587) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:51:30.797+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:51:30.797+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:51:30.797+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:51:30.866+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:51:30.887+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:51:30.887+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:51:30.909+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:51:30.909+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:51:00+00:00, run_after=2024-03-19 10:52:00+00:00
[2024-03-19T10:51:30.925+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.134 seconds
[2024-03-19T10:52:01.213+0000] {processor.py:161} INFO - Started process (PID=9644) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:52:01.214+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:52:01.216+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:52:01.215+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:52:01.286+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:52:01.303+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:52:01.303+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:52:01.453+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:52:01.452+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:52:00+00:00, run_after=2024-03-19 10:53:00+00:00
[2024-03-19T10:52:01.465+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.256 seconds
[2024-03-19T10:52:31.632+0000] {processor.py:161} INFO - Started process (PID=9701) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:52:31.633+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:52:31.634+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:52:31.634+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:52:31.701+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:52:31.719+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:52:31.718+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:52:31.740+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:52:31.740+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:52:00+00:00, run_after=2024-03-19 10:53:00+00:00
[2024-03-19T10:52:31.972+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.343 seconds
[2024-03-19T10:53:02.220+0000] {processor.py:161} INFO - Started process (PID=9758) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:53:02.221+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:53:02.222+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:53:02.222+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:53:02.281+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:53:02.296+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:53:02.296+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:53:02.314+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:53:02.314+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:53:00+00:00, run_after=2024-03-19 10:54:00+00:00
[2024-03-19T10:53:02.328+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.110 seconds
[2024-03-19T10:53:32.700+0000] {processor.py:161} INFO - Started process (PID=9815) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:53:32.702+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:53:32.703+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:53:32.702+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:53:32.770+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:53:33.004+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:53:33.004+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:53:33.026+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:53:33.026+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:53:00+00:00, run_after=2024-03-19 10:54:00+00:00
[2024-03-19T10:53:33.042+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.345 seconds
[2024-03-19T10:54:03.118+0000] {processor.py:161} INFO - Started process (PID=9872) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:54:03.119+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:54:03.120+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:54:03.120+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:54:03.183+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:54:03.199+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:54:03.199+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:54:03.352+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:54:03.352+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:54:00+00:00, run_after=2024-03-19 10:55:00+00:00
[2024-03-19T10:54:03.364+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.249 seconds
[2024-03-19T10:54:33.745+0000] {processor.py:161} INFO - Started process (PID=9929) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:54:33.746+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:54:33.747+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:54:33.747+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:54:33.814+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:54:33.831+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:54:33.830+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:54:33.981+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:54:33.980+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:54:00+00:00, run_after=2024-03-19 10:55:00+00:00
[2024-03-19T10:54:33.995+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.252 seconds
[2024-03-19T10:55:04.420+0000] {processor.py:161} INFO - Started process (PID=9986) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:55:04.421+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:55:04.422+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:55:04.422+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:55:04.493+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:55:04.509+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:55:04.509+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:55:04.528+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:55:04.528+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:55:00+00:00, run_after=2024-03-19 10:56:00+00:00
[2024-03-19T10:55:04.543+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.125 seconds
[2024-03-19T10:55:34.856+0000] {processor.py:161} INFO - Started process (PID=10043) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:55:34.857+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:55:34.858+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:55:34.858+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:55:34.948+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:55:35.213+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:55:35.212+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:55:35.237+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:55:35.237+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:55:00+00:00, run_after=2024-03-19 10:56:00+00:00
[2024-03-19T10:55:35.259+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.407 seconds
[2024-03-19T10:56:05.352+0000] {processor.py:161} INFO - Started process (PID=10102) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:56:05.353+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:56:05.354+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:56:05.354+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:56:05.434+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:56:05.452+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:56:05.452+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:56:05.696+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:56:05.696+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:56:00+00:00, run_after=2024-03-19 10:57:00+00:00
[2024-03-19T10:56:05.712+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.363 seconds
[2024-03-19T10:56:35.787+0000] {processor.py:161} INFO - Started process (PID=10156) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:56:35.788+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:56:35.789+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:56:35.788+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:56:35.861+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:56:35.878+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:56:35.878+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:56:36.119+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:56:36.119+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:56:00+00:00, run_after=2024-03-19 10:57:00+00:00
[2024-03-19T10:56:36.134+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.350 seconds
[2024-03-19T10:57:06.632+0000] {processor.py:161} INFO - Started process (PID=10213) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:57:06.633+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:57:06.634+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:57:06.634+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:57:06.843+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:57:06.857+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:57:06.857+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:57:06.874+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:57:06.874+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:57:00+00:00, run_after=2024-03-19 10:58:00+00:00
[2024-03-19T10:57:06.888+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.259 seconds
[2024-03-19T10:57:37.030+0000] {processor.py:161} INFO - Started process (PID=10270) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:57:37.031+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:57:37.032+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:57:37.031+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:57:37.108+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:57:37.353+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:57:37.353+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:57:37.374+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:57:37.374+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:57:00+00:00, run_after=2024-03-19 10:58:00+00:00
[2024-03-19T10:57:37.389+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.362 seconds
[2024-03-19T10:58:07.824+0000] {processor.py:161} INFO - Started process (PID=10327) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:58:07.825+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:58:07.826+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:58:07.826+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:58:07.907+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:58:07.927+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:58:07.927+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:58:08.182+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:58:08.182+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:58:00+00:00, run_after=2024-03-19 10:59:00+00:00
[2024-03-19T10:58:08.198+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.378 seconds
[2024-03-19T10:58:38.317+0000] {processor.py:161} INFO - Started process (PID=10387) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:58:38.318+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:58:38.318+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:58:38.318+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:58:38.600+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:58:38.617+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:58:38.616+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:58:38.637+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:58:38.637+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:58:00+00:00, run_after=2024-03-19 10:59:00+00:00
[2024-03-19T10:58:38.653+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.339 seconds
[2024-03-19T10:59:08.962+0000] {processor.py:161} INFO - Started process (PID=10441) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:59:08.963+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:59:08.964+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:59:08.964+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:59:09.252+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:59:09.267+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:59:09.267+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:59:09.286+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:59:09.286+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:59:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:59:09.304+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.346 seconds
[2024-03-19T10:59:39.398+0000] {processor.py:161} INFO - Started process (PID=10501) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T10:59:39.399+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T10:59:39.400+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:59:39.400+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:59:39.683+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T10:59:39.701+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:59:39.701+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T10:59:39.719+0000] {logging_mixin.py:188} INFO - [2024-03-19T10:59:39.719+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 10:59:00+00:00, run_after=2024-03-19 11:00:00+00:00
[2024-03-19T10:59:39.735+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.340 seconds
[2024-03-19T11:00:10.666+0000] {processor.py:161} INFO - Started process (PID=10559) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:00:10.667+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:00:10.668+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:00:10.667+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:00:10.946+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:00:10.965+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:00:10.964+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:00:10.982+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:00:10.982+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:00:00+00:00, run_after=2024-03-19 11:01:00+00:00
[2024-03-19T11:00:11.001+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.339 seconds
[2024-03-19T11:00:41.082+0000] {processor.py:161} INFO - Started process (PID=10619) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:00:41.083+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:00:41.084+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:00:41.084+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:00:41.381+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:00:41.396+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:00:41.396+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:00:41.417+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:00:41.416+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:00:00+00:00, run_after=2024-03-19 11:01:00+00:00
[2024-03-19T11:00:41.430+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.351 seconds
[2024-03-19T11:00:50.498+0000] {processor.py:161} INFO - Started process (PID=10641) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:00:50.499+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:00:50.501+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:00:50.501+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:00:50.751+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:00:50.768+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:00:50.768+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:00:50.785+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:00:50.785+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:00:00+00:00, run_after=2024-03-19 11:01:00+00:00
[2024-03-19T11:00:50.800+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.306 seconds
[2024-03-19T11:00:56.575+0000] {processor.py:161} INFO - Started process (PID=10643) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:00:56.576+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:00:56.576+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:00:56.576+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:00:56.775+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:00:56.789+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:00:56.788+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:00:56.806+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:00:56.806+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:00:00+00:00, run_after=2024-03-19 11:01:00+00:00
[2024-03-19T11:00:56.821+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.250 seconds
[2024-03-19T11:01:27.647+0000] {processor.py:161} INFO - Started process (PID=10699) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:01:27.648+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:01:27.649+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:01:27.648+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:01:27.827+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:01:27.844+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:01:27.843+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:01:27.865+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:01:27.865+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:01:00+00:00, run_after=2024-03-19 11:02:00+00:00
[2024-03-19T11:01:27.880+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.236 seconds
[2024-03-19T11:01:50.879+0000] {processor.py:161} INFO - Started process (PID=10755) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:01:50.881+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:01:50.881+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:01:50.881+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:01:51.130+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:01:51.145+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:01:51.144+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:01:51.164+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:01:51.164+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:01:00+00:00, run_after=2024-03-19 11:02:00+00:00
[2024-03-19T11:01:51.180+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.304 seconds
[2024-03-19T11:01:58.183+0000] {processor.py:161} INFO - Started process (PID=10757) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:01:58.184+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:01:58.185+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:01:58.184+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:01:58.364+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:01:58.379+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:01:58.378+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:01:58.397+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:01:58.397+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:01:00+00:00, run_after=2024-03-19 11:02:00+00:00
[2024-03-19T11:01:58.409+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.229 seconds
[2024-03-19T11:02:28.905+0000] {processor.py:161} INFO - Started process (PID=10814) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:02:28.906+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:02:28.907+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:02:28.907+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:02:29.108+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:02:29.123+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:02:29.122+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:02:29.142+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:02:29.142+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:02:00+00:00, run_after=2024-03-19 11:03:00+00:00
[2024-03-19T11:02:29.159+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.257 seconds
[2024-03-19T11:03:29.704+0000] {processor.py:161} INFO - Started process (PID=43) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:03:29.705+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:03:29.707+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:03:29.707+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:03:29.888+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:03:30.064+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:03:30.063+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:03:30.081+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:03:30.081+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:02:00+00:00, run_after=2024-03-19 11:03:00+00:00
[2024-03-19T11:03:30.098+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.396 seconds
[2024-03-19T11:03:30.940+0000] {processor.py:161} INFO - Started process (PID=61) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:03:30.942+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:03:30.944+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:03:30.944+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:03:31.029+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:03:31.042+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:03:31.041+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:03:31.069+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:03:31.069+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:03:00+00:00, run_after=2024-03-19 11:04:00+00:00
[2024-03-19T11:03:31.105+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.168 seconds
[2024-03-19T11:04:01.942+0000] {processor.py:161} INFO - Started process (PID=117) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:04:01.943+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:04:01.944+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:04:01.944+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:04:02.029+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:04:02.045+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:04:02.045+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:04:02.063+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:04:02.063+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:04:00+00:00, run_after=2024-03-19 11:05:00+00:00
[2024-03-19T11:04:02.077+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.138 seconds
[2024-03-19T11:04:21.457+0000] {processor.py:161} INFO - Started process (PID=157) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:04:21.458+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:04:21.460+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:04:21.459+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:04:21.556+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:04:21.575+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:04:21.574+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:04:21.600+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:04:21.600+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:04:00+00:00, run_after=2024-03-19 11:05:00+00:00
[2024-03-19T11:04:21.651+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.198 seconds
[2024-03-19T11:04:31.667+0000] {processor.py:161} INFO - Started process (PID=161) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:04:31.668+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:04:31.670+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:04:31.669+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:04:31.757+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:04:31.774+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:04:31.773+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:04:31.794+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:04:31.793+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:04:00+00:00, run_after=2024-03-19 11:05:00+00:00
[2024-03-19T11:04:31.810+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.147 seconds
[2024-03-19T11:04:50.186+0000] {processor.py:161} INFO - Started process (PID=215) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:04:50.188+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:04:50.190+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:04:50.189+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:04:50.261+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:04:50.281+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:04:50.281+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:04:50.303+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:04:50.303+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:04:00+00:00, run_after=2024-03-19 11:05:00+00:00
[2024-03-19T11:04:50.323+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.140 seconds
[2024-03-19T11:04:56.809+0000] {processor.py:161} INFO - Started process (PID=216) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:04:56.814+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:04:56.816+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:04:56.815+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:04:56.887+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:04:56.905+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:04:56.905+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:04:56.924+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:04:56.924+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:04:00+00:00, run_after=2024-03-19 11:05:00+00:00
[2024-03-19T11:04:56.938+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.132 seconds
[2024-03-19T11:05:27.528+0000] {processor.py:161} INFO - Started process (PID=273) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:05:27.529+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:05:27.531+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:05:27.531+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:05:27.593+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:05:27.612+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:05:27.611+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:05:27.630+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:05:27.630+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:05:00+00:00, run_after=2024-03-19 11:06:00+00:00
[2024-03-19T11:05:27.646+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.121 seconds
[2024-03-19T11:05:50.464+0000] {processor.py:161} INFO - Started process (PID=330) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:05:50.466+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:05:50.468+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:05:50.467+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:05:50.534+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:05:50.553+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:05:50.552+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:05:50.571+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:05:50.571+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:05:00+00:00, run_after=2024-03-19 11:06:00+00:00
[2024-03-19T11:05:50.585+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.125 seconds
[2024-03-19T11:05:57.415+0000] {processor.py:161} INFO - Started process (PID=331) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:05:57.417+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:05:57.420+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:05:57.420+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:05:57.504+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:05:57.524+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:05:57.523+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:05:57.545+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:05:57.544+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:05:00+00:00, run_after=2024-03-19 11:06:00+00:00
[2024-03-19T11:05:57.561+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.150 seconds
[2024-03-19T11:06:28.008+0000] {processor.py:161} INFO - Started process (PID=388) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:06:28.009+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:06:28.011+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:06:28.011+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:06:28.072+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:06:28.091+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:06:28.090+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:06:28.110+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:06:28.110+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:06:00+00:00, run_after=2024-03-19 11:07:00+00:00
[2024-03-19T11:06:28.127+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.121 seconds
[2024-03-19T11:06:50.847+0000] {processor.py:161} INFO - Started process (PID=445) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:06:50.848+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:06:50.851+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:06:50.851+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:06:50.913+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:06:50.931+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:06:50.931+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:06:50.951+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:06:50.951+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:06:00+00:00, run_after=2024-03-19 11:07:00+00:00
[2024-03-19T11:06:50.966+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.123 seconds
[2024-03-19T11:06:56.943+0000] {processor.py:161} INFO - Started process (PID=446) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:06:56.944+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:06:56.946+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:06:56.945+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:06:57.018+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:06:57.035+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:06:57.035+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:06:57.055+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:06:57.054+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:06:00+00:00, run_after=2024-03-19 11:07:00+00:00
[2024-03-19T11:06:57.068+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.130 seconds
[2024-03-19T11:07:27.272+0000] {processor.py:161} INFO - Started process (PID=503) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:07:27.273+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:07:27.274+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:07:27.274+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:07:27.334+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:07:27.354+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:07:27.353+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:07:27.372+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:07:27.372+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:07:00+00:00, run_after=2024-03-19 11:08:00+00:00
[2024-03-19T11:07:27.387+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.119 seconds
[2024-03-19T11:07:50.593+0000] {processor.py:161} INFO - Started process (PID=560) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:07:50.595+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:07:50.597+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:07:50.597+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:07:50.657+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:07:50.678+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:07:50.677+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:07:50.697+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:07:50.697+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:07:00+00:00, run_after=2024-03-19 11:08:00+00:00
[2024-03-19T11:07:50.714+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.124 seconds
[2024-03-19T11:07:56.668+0000] {processor.py:161} INFO - Started process (PID=561) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:07:56.670+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:07:56.673+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:07:56.672+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:07:56.765+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:07:56.790+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:07:56.789+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:07:56.810+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:07:56.809+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:07:00+00:00, run_after=2024-03-19 11:08:00+00:00
[2024-03-19T11:07:56.825+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.162 seconds
[2024-03-19T11:08:27.073+0000] {processor.py:161} INFO - Started process (PID=618) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:08:27.074+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:08:27.076+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:08:27.076+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:08:27.138+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:08:27.156+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:08:27.156+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:08:27.175+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:08:27.175+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:08:00+00:00, run_after=2024-03-19 11:09:00+00:00
[2024-03-19T11:08:27.189+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.120 seconds
[2024-03-19T11:08:51.192+0000] {processor.py:161} INFO - Started process (PID=675) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:08:51.193+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:08:51.196+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:08:51.195+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:08:51.255+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:08:51.273+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:08:51.272+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:08:51.293+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:08:51.293+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:08:00+00:00, run_after=2024-03-19 11:09:00+00:00
[2024-03-19T11:08:51.309+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.120 seconds
[2024-03-19T11:08:57.260+0000] {processor.py:161} INFO - Started process (PID=676) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:08:57.261+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:08:57.263+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:08:57.263+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:08:57.330+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:08:57.351+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:08:57.351+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:08:57.371+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:08:57.371+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:08:00+00:00, run_after=2024-03-19 11:09:00+00:00
[2024-03-19T11:08:57.387+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.131 seconds
[2024-03-19T11:09:27.738+0000] {processor.py:161} INFO - Started process (PID=733) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:09:27.739+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:09:27.741+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:09:27.741+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:09:27.802+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:09:27.822+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:09:27.822+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:09:27.842+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:09:27.842+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:09:00+00:00, run_after=2024-03-19 11:10:00+00:00
[2024-03-19T11:09:27.859+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.123 seconds
[2024-03-19T11:09:50.751+0000] {processor.py:161} INFO - Started process (PID=790) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:09:50.753+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:09:50.755+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:09:50.754+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:09:50.816+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:09:50.835+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:09:50.834+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:09:50.854+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:09:50.853+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:09:00+00:00, run_after=2024-03-19 11:10:00+00:00
[2024-03-19T11:09:50.869+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.121 seconds
[2024-03-19T11:09:56.827+0000] {processor.py:161} INFO - Started process (PID=791) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:09:56.828+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:09:56.831+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:09:56.830+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:09:56.904+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:09:56.924+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:09:56.923+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:09:56.943+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:09:56.942+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:09:00+00:00, run_after=2024-03-19 11:10:00+00:00
[2024-03-19T11:09:56.959+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.136 seconds
[2024-03-19T11:10:27.414+0000] {processor.py:161} INFO - Started process (PID=848) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:10:27.415+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:10:27.417+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:10:27.417+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:10:27.475+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:10:27.495+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:10:27.494+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:10:27.513+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:10:27.513+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:10:00+00:00, run_after=2024-03-19 11:11:00+00:00
[2024-03-19T11:10:27.528+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.117 seconds
[2024-03-19T11:10:51.347+0000] {processor.py:161} INFO - Started process (PID=905) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:10:51.348+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:10:51.349+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:10:51.349+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:10:51.408+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:10:51.427+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:10:51.427+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:10:51.447+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:10:51.446+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:10:00+00:00, run_after=2024-03-19 11:11:00+00:00
[2024-03-19T11:10:51.461+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.118 seconds
[2024-03-19T11:10:57.428+0000] {processor.py:161} INFO - Started process (PID=906) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:10:57.429+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:10:57.431+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:10:57.430+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:10:57.495+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:10:57.513+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:10:57.512+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:10:57.532+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:10:57.532+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:10:00+00:00, run_after=2024-03-19 11:11:00+00:00
[2024-03-19T11:10:57.548+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.123 seconds
[2024-03-19T11:34:18.154+0000] {processor.py:161} INFO - Started process (PID=970) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:34:18.157+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:34:18.165+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:34:18.165+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:34:18.364+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:34:18.406+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:34:18.405+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:34:18.452+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:34:18.452+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:12:00+00:00, run_after=2024-03-19 11:13:00+00:00
[2024-03-19T11:34:18.489+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.342 seconds
[2024-03-19T11:34:33.571+0000] {processor.py:161} INFO - Started process (PID=1020) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:34:33.572+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:34:33.575+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:34:33.574+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:34:33.692+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:34:33.726+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:34:33.726+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:34:33.760+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:34:33.760+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:25:00+00:00, run_after=2024-03-19 11:26:00+00:00
[2024-03-19T11:34:33.786+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.218 seconds
[2024-03-19T11:34:53.674+0000] {processor.py:161} INFO - Started process (PID=1030) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:34:53.676+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:34:53.682+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:34:53.680+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:34:53.934+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:34:53.991+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:34:53.991+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-19T11:14:00+00:00', 'Hostname': '26a0b219297f', 'External Executor Id': 'f5197be0-ff6f-484c-a93a-173536971232'}
[2024-03-19T11:34:54.063+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:34:54.063+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240319T111400, start_date=20240319T113428, end_date=20240319T113454
[2024-03-19T11:34:54.085+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-19T11:14:00+00:00 [failed]> in state failed
[2024-03-19T11:34:54.109+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:34:54.108+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:34:54.162+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.494 seconds
[2024-03-19T11:35:12.128+0000] {processor.py:161} INFO - Started process (PID=1057) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:35:12.129+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:35:12.131+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:35:12.130+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:35:12.378+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:35:12.405+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:35:12.404+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-19T11:15:00+00:00', 'Hostname': '26a0b219297f', 'External Executor Id': 'a088a3aa-947d-4b49-8ee1-db2fa17d1e84'}
[2024-03-19T11:35:12.459+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:35:12.459+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240319T111500, start_date=20240319T113428, end_date=20240319T113512
[2024-03-19T11:35:27.436+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-19T11:15:00+00:00 [failed]> in state failed
[2024-03-19T11:35:27.440+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:35:27.440+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-19T11:16:00+00:00', 'Hostname': '26a0b219297f', 'External Executor Id': 'ae201262-6378-4c1b-890b-a39754dcdfe2'}
[2024-03-19T11:35:27.458+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:35:27.458+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240319T111600, start_date=20240319T113428, end_date=20240319T113527
[2024-03-19T11:35:27.464+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-19T11:16:00+00:00 [failed]> in state failed
[2024-03-19T11:35:27.468+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:35:27.468+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-19T11:13:00+00:00', 'Hostname': '26a0b219297f', 'External Executor Id': 'b641d05a-d88c-4f68-8395-12b9b0ae3d4e'}
[2024-03-19T11:35:27.481+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:35:27.481+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240319T111300, start_date=20240319T113428, end_date=20240319T113527
[2024-03-19T11:35:27.484+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: Crawler.your_task_id scheduled__2024-03-19T11:13:00+00:00 [failed]> in state failed
[2024-03-19T11:35:27.486+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:35:27.486+0000] {taskinstance.py:2733} ERROR - {'DAG Id': 'Crawler', 'Task Id': 'your_task_id', 'Run Id': 'scheduled__2024-03-19T11:15:00+00:00', 'Hostname': '26a0b219297f', 'External Executor Id': 'a088a3aa-947d-4b49-8ee1-db2fa17d1e84'}
[2024-03-19T11:35:27.497+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:35:27.496+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=Crawler, task_id=your_task_id, execution_date=20240319T111500, start_date=20240319T113428, end_date=20240319T113527
[2024-03-19T11:35:28.503+0000] {processor.py:715} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.DeadlockDetected: deadlock detected
DETAIL:  Process 2713 waits for ShareLock on transaction 98893; blocked by process 2164.
Process 2164 waits for ShareLock on transaction 98882; blocked by process 2713.
HINT:  See server log for query details.
CONTEXT:  while locking tuple (5,182) in relation "dag_run"
SQL statement "SELECT 1 FROM ONLY "public"."dag_run" x WHERE "dag_id"::pg_catalog.text OPERATOR(pg_catalog.=) $1::pg_catalog.text AND "run_id"::pg_catalog.text OPERATOR(pg_catalog.=) $2::pg_catalog.text FOR KEY SHARE OF x"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 709, in execute_callbacks
    self._execute_task_callbacks(dagbag, request, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 790, in _execute_task_callbacks
    ti.handle_failure(error=request.msg, test_mode=self.UNIT_TEST_MODE, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2827, in handle_failure
    _handle_failure(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 846, in _handle_failure
    TaskInstance.save_to_db(failure_context["ti"], session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2807, in save_to_db
    session.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 3589, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/persistence.py", line 237, in save_obj
    _emit_update_statements(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/persistence.py", line 1001, in _emit_update_statements
    c = connection._execute_20(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (psycopg2.errors.DeadlockDetected) deadlock detected
DETAIL:  Process 2713 waits for ShareLock on transaction 98893; blocked by process 2164.
Process 2164 waits for ShareLock on transaction 98882; blocked by process 2713.
HINT:  See server log for query details.
CONTEXT:  while locking tuple (5,182) in relation "dag_run"
SQL statement "SELECT 1 FROM ONLY "public"."dag_run" x WHERE "dag_id"::pg_catalog.text OPERATOR(pg_catalog.=) $1::pg_catalog.text AND "run_id"::pg_catalog.text OPERATOR(pg_catalog.=) $2::pg_catalog.text FOR KEY SHARE OF x"

[SQL: UPDATE task_instance SET end_date=%(end_date)s, duration=%(duration)s, updated_at=%(updated_at)s WHERE task_instance.dag_id = %(task_instance_dag_id)s AND task_instance.task_id = %(task_instance_task_id)s AND task_instance.run_id = %(task_instance_run_id)s AND task_instance.map_index = %(task_instance_map_index)s]
[parameters: {'end_date': datetime.datetime(2024, 3, 19, 11, 35, 27, 489738, tzinfo=Timezone('UTC')), 'duration': 58.74884, 'updated_at': datetime.datetime(2024, 3, 19, 11, 35, 27, 500095, tzinfo=Timezone('UTC')), 'task_instance_dag_id': 'Crawler', 'task_instance_task_id': 'your_task_id', 'task_instance_run_id': 'scheduled__2024-03-19T11:15:00+00:00', 'task_instance_map_index': -1}]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2024-03-19T11:35:28.511+0000] {processor.py:715} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 709, in execute_callbacks
    self._execute_task_callbacks(dagbag, request, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 762, in _execute_task_callbacks
    ti = TaskInstance.get_task_instance(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1734, in get_task_instance
    return query.one_or_none()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2850, in one_or_none
    return self._iter().one_or_none()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.DeadlockDetected) deadlock detected
DETAIL:  Process 2713 waits for ShareLock on transaction 98893; blocked by process 2164.
Process 2164 waits for ShareLock on transaction 98882; blocked by process 2713.
HINT:  See server log for query details.
CONTEXT:  while locking tuple (5,182) in relation "dag_run"
SQL statement "SELECT 1 FROM ONLY "public"."dag_run" x WHERE "dag_id"::pg_catalog.text OPERATOR(pg_catalog.=) $1::pg_catalog.text AND "run_id"::pg_catalog.text OPERATOR(pg_catalog.=) $2::pg_catalog.text FOR KEY SHARE OF x"

[SQL: UPDATE task_instance SET end_date=%(end_date)s, duration=%(duration)s, updated_at=%(updated_at)s WHERE task_instance.dag_id = %(task_instance_dag_id)s AND task_instance.task_id = %(task_instance_task_id)s AND task_instance.run_id = %(task_instance_run_id)s AND task_instance.map_index = %(task_instance_map_index)s]
[parameters: {'end_date': datetime.datetime(2024, 3, 19, 11, 35, 27, 489738, tzinfo=Timezone('UTC')), 'duration': 58.74884, 'updated_at': datetime.datetime(2024, 3, 19, 11, 35, 27, 500095, tzinfo=Timezone('UTC')), 'task_instance_dag_id': 'Crawler', 'task_instance_task_id': 'your_task_id', 'task_instance_run_id': 'scheduled__2024-03-19T11:15:00+00:00', 'task_instance_map_index': -1}]
(Background on this error at: https://sqlalche.me/e/14/e3q8) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-03-19T11:35:28.513+0000] {processor.py:715} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 709, in execute_callbacks
    self._execute_task_callbacks(dagbag, request, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 762, in _execute_task_callbacks
    ti = TaskInstance.get_task_instance(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1734, in get_task_instance
    return query.one_or_none()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2850, in one_or_none
    return self._iter().one_or_none()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.DeadlockDetected) deadlock detected
DETAIL:  Process 2713 waits for ShareLock on transaction 98893; blocked by process 2164.
Process 2164 waits for ShareLock on transaction 98882; blocked by process 2713.
HINT:  See server log for query details.
CONTEXT:  while locking tuple (5,182) in relation "dag_run"
SQL statement "SELECT 1 FROM ONLY "public"."dag_run" x WHERE "dag_id"::pg_catalog.text OPERATOR(pg_catalog.=) $1::pg_catalog.text AND "run_id"::pg_catalog.text OPERATOR(pg_catalog.=) $2::pg_catalog.text FOR KEY SHARE OF x"

[SQL: UPDATE task_instance SET end_date=%(end_date)s, duration=%(duration)s, updated_at=%(updated_at)s WHERE task_instance.dag_id = %(task_instance_dag_id)s AND task_instance.task_id = %(task_instance_task_id)s AND task_instance.run_id = %(task_instance_run_id)s AND task_instance.map_index = %(task_instance_map_index)s]
[parameters: {'end_date': datetime.datetime(2024, 3, 19, 11, 35, 27, 489738, tzinfo=Timezone('UTC')), 'duration': 58.74884, 'updated_at': datetime.datetime(2024, 3, 19, 11, 35, 27, 500095, tzinfo=Timezone('UTC')), 'task_instance_dag_id': 'Crawler', 'task_instance_task_id': 'your_task_id', 'task_instance_run_id': 'scheduled__2024-03-19T11:15:00+00:00', 'task_instance_map_index': -1}]
(Background on this error at: https://sqlalche.me/e/14/e3q8) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-03-19T11:35:28.513+0000] {processor.py:715} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 709, in execute_callbacks
    self._execute_task_callbacks(dagbag, request, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 762, in _execute_task_callbacks
    ti = TaskInstance.get_task_instance(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1734, in get_task_instance
    return query.one_or_none()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2850, in one_or_none
    return self._iter().one_or_none()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.DeadlockDetected) deadlock detected
DETAIL:  Process 2713 waits for ShareLock on transaction 98893; blocked by process 2164.
Process 2164 waits for ShareLock on transaction 98882; blocked by process 2713.
HINT:  See server log for query details.
CONTEXT:  while locking tuple (5,182) in relation "dag_run"
SQL statement "SELECT 1 FROM ONLY "public"."dag_run" x WHERE "dag_id"::pg_catalog.text OPERATOR(pg_catalog.=) $1::pg_catalog.text AND "run_id"::pg_catalog.text OPERATOR(pg_catalog.=) $2::pg_catalog.text FOR KEY SHARE OF x"

[SQL: UPDATE task_instance SET end_date=%(end_date)s, duration=%(duration)s, updated_at=%(updated_at)s WHERE task_instance.dag_id = %(task_instance_dag_id)s AND task_instance.task_id = %(task_instance_task_id)s AND task_instance.run_id = %(task_instance_run_id)s AND task_instance.map_index = %(task_instance_map_index)s]
[parameters: {'end_date': datetime.datetime(2024, 3, 19, 11, 35, 27, 489738, tzinfo=Timezone('UTC')), 'duration': 58.74884, 'updated_at': datetime.datetime(2024, 3, 19, 11, 35, 27, 500095, tzinfo=Timezone('UTC')), 'task_instance_dag_id': 'Crawler', 'task_instance_task_id': 'your_task_id', 'task_instance_run_id': 'scheduled__2024-03-19T11:15:00+00:00', 'task_instance_map_index': -1}]
(Background on this error at: https://sqlalche.me/e/14/e3q8) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-03-19T11:35:28.514+0000] {processor.py:715} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/new_crawler.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 709, in execute_callbacks
    self._execute_task_callbacks(dagbag, request, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 762, in _execute_task_callbacks
    ti = TaskInstance.get_task_instance(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1734, in get_task_instance
    return query.one_or_none()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2850, in one_or_none
    return self._iter().one_or_none()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.DeadlockDetected) deadlock detected
DETAIL:  Process 2713 waits for ShareLock on transaction 98893; blocked by process 2164.
Process 2164 waits for ShareLock on transaction 98882; blocked by process 2713.
HINT:  See server log for query details.
CONTEXT:  while locking tuple (5,182) in relation "dag_run"
SQL statement "SELECT 1 FROM ONLY "public"."dag_run" x WHERE "dag_id"::pg_catalog.text OPERATOR(pg_catalog.=) $1::pg_catalog.text AND "run_id"::pg_catalog.text OPERATOR(pg_catalog.=) $2::pg_catalog.text FOR KEY SHARE OF x"

[SQL: UPDATE task_instance SET end_date=%(end_date)s, duration=%(duration)s, updated_at=%(updated_at)s WHERE task_instance.dag_id = %(task_instance_dag_id)s AND task_instance.task_id = %(task_instance_task_id)s AND task_instance.run_id = %(task_instance_run_id)s AND task_instance.map_index = %(task_instance_map_index)s]
[parameters: {'end_date': datetime.datetime(2024, 3, 19, 11, 35, 27, 489738, tzinfo=Timezone('UTC')), 'duration': 58.74884, 'updated_at': datetime.datetime(2024, 3, 19, 11, 35, 27, 500095, tzinfo=Timezone('UTC')), 'task_instance_dag_id': 'Crawler', 'task_instance_task_id': 'your_task_id', 'task_instance_run_id': 'scheduled__2024-03-19T11:15:00+00:00', 'task_instance_map_index': -1}]
(Background on this error at: https://sqlalche.me/e/14/e3q8) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-03-19T11:35:28.515+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    session.commit()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 830, in commit
    self._assert_active(prepared_ok=True)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.DeadlockDetected) deadlock detected
DETAIL:  Process 2713 waits for ShareLock on transaction 98893; blocked by process 2164.
Process 2164 waits for ShareLock on transaction 98882; blocked by process 2713.
HINT:  See server log for query details.
CONTEXT:  while locking tuple (5,182) in relation "dag_run"
SQL statement "SELECT 1 FROM ONLY "public"."dag_run" x WHERE "dag_id"::pg_catalog.text OPERATOR(pg_catalog.=) $1::pg_catalog.text AND "run_id"::pg_catalog.text OPERATOR(pg_catalog.=) $2::pg_catalog.text FOR KEY SHARE OF x"

[SQL: UPDATE task_instance SET end_date=%(end_date)s, duration=%(duration)s, updated_at=%(updated_at)s WHERE task_instance.dag_id = %(task_instance_dag_id)s AND task_instance.task_id = %(task_instance_task_id)s AND task_instance.run_id = %(task_instance_run_id)s AND task_instance.map_index = %(task_instance_map_index)s]
[parameters: {'end_date': datetime.datetime(2024, 3, 19, 11, 35, 27, 489738, tzinfo=Timezone('UTC')), 'duration': 58.74884, 'updated_at': datetime.datetime(2024, 3, 19, 11, 35, 27, 500095, tzinfo=Timezone('UTC')), 'task_instance_dag_id': 'Crawler', 'task_instance_task_id': 'your_task_id', 'task_instance_run_id': 'scheduled__2024-03-19T11:15:00+00:00', 'task_instance_map_index': -1}]
(Background on this error at: https://sqlalche.me/e/14/e3q8) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-03-19T11:35:28.557+0000] {processor.py:161} INFO - Started process (PID=1081) to work on /opt/airflow/dags/new_crawler.py
[2024-03-19T11:35:28.558+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/new_crawler.py for tasks to queue
[2024-03-19T11:35:28.560+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:35:28.560+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:35:28.644+0000] {processor.py:840} INFO - DAG(s) 'Crawler' retrieved from /opt/airflow/dags/new_crawler.py
[2024-03-19T11:35:28.661+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:35:28.661+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-03-19T11:35:28.677+0000] {logging_mixin.py:188} INFO - [2024-03-19T11:35:28.677+0000] {dag.py:3834} INFO - Setting next_dagrun for Crawler to 2024-03-19 11:28:00+00:00, run_after=2024-03-19 11:29:00+00:00
[2024-03-19T11:35:28.689+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/new_crawler.py took 0.134 seconds
